{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "import io\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f716583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flipkart_urls(index_url: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Extract all product URLs from a given Flipkart sitemap index XML.\n",
    "\n",
    "    Args:\n",
    "        index_url (str): Flipkart sitemap index URL (e.g. 'https://www.flipkart.com/sitemap_p_product_index_1.xml')\n",
    "        output_path (str): Path to save the extracted URLs (e.g. './data/urls_index_1.csv')\n",
    "    \"\"\"\n",
    "\n",
    "    # print which sitemap index file is being processed\n",
    "    print(f\"Processing sitemap index: {index_url}\")\n",
    "    \n",
    "    # initialize an empty list to store all product URLs found\n",
    "    all_product_urls = []\n",
    "\n",
    "    try:\n",
    "        # send a GET request to fetch the sitemap index XML file\n",
    "        resp = requests.get(index_url, timeout=15)\n",
    "        \n",
    "        # raise an error if the request was not successful (non-200 status code)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        # parse XML content of the sitemap index file\n",
    "        root = ET.fromstring(resp.content)\n",
    "        \n",
    "        # define XML namespace mapping used in Flipkartâ€™s sitemap format\n",
    "        ns = {\"sm\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
    "\n",
    "        # iterate over each <sitemap> element inside the sitemap index\n",
    "        for sitemap in root.findall(\"sm:sitemap\", ns):\n",
    "            \n",
    "            # extract <loc> tag inside <sitemap>, which contains the URL of a gzipped sitemap file\n",
    "            loc_tag = sitemap.find(\"sm:loc\", ns)\n",
    "            \n",
    "            # skip if no <loc> tag is found (invalid or empty sitemap entry)\n",
    "            if loc_tag is None:\n",
    "                continue\n",
    "\n",
    "            # extract actual URL (link to a .xml.gz file)\n",
    "            gz_url = loc_tag.text\n",
    "            print(f\"Found sitemap: {gz_url}\")\n",
    "\n",
    "            try:\n",
    "                # send GET request to fetch the gzipped XML file\n",
    "                gz_resp = requests.get(gz_url, timeout=30)\n",
    "                \n",
    "                # raise an exception if the download failed\n",
    "                gz_resp.raise_for_status()\n",
    "\n",
    "                # decompress the .gz file content using gzip\n",
    "                with gzip.GzipFile(fileobj=io.BytesIO(gz_resp.content)) as gz_file:\n",
    "                    xml_content = gz_file.read()\n",
    "\n",
    "                # parse the decompressed XML content to extract individual URLs\n",
    "                url_root = ET.fromstring(xml_content)\n",
    "                \n",
    "                # loop over all <url> elements in the sitemap\n",
    "                for url_tag in url_root.findall(\"sm:url\", ns):\n",
    "                    \n",
    "                    # extract the <loc> tag within each <url> tag\n",
    "                    loc_inner = url_tag.find(\"sm:loc\", ns)\n",
    "                    \n",
    "                    # if <loc> exists, extract the actual product URL text\n",
    "                    if loc_inner is not None:\n",
    "                        all_product_urls.append(loc_inner.text)\n",
    "            \n",
    "            # handle exceptions that occur while parsing or downloading individual gzipped sitemaps\n",
    "            except Exception as inner_e:\n",
    "                print(f\"Failed to parse {gz_url}: {inner_e}\")\n",
    "                continue\n",
    "\n",
    "    # handle exceptions that occur while fetching or parsing the main sitemap index\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process index file {index_url}: {e}\")\n",
    "        return\n",
    "\n",
    "    # print the total number of URLs successfully extracted\n",
    "    print(f\"\\nTotal URLs extracted: {len(all_product_urls)}\")\n",
    "    \n",
    "    # open the output CSV file in write mode (UTF-8 encoded)\n",
    "    with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        # create a CSV writer object\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # write header row to the CSV file\n",
    "        writer.writerow([\"URL\"])\n",
    "        \n",
    "        # write each extracted product URL on a new line\n",
    "        for url in all_product_urls:\n",
    "            writer.writerow([url])\n",
    "\n",
    "    # confirm that the URLs have been saved successfully\n",
    "    print(f\"Saved URLs to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    extract_flipkart_urls(\n",
    "        index_url=f\"https://www.flipkart.com/sitemap_p_product_index_{i+1}.xml\",\n",
    "        output_path=f\"./data/flipkart_urls_index_{i+1}.csv\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds605-gproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# for sending HTTP requests to Flipkart pages\n",
    "import requests\n",
    "\n",
    "# for loading/saving URL lists and scraped results\n",
    "import pandas as pd\n",
    "\n",
    "# for timing execution and sleep delays\n",
    "import time\n",
    "\n",
    "# for random sleep intervals (avoid rate-limiting)\n",
    "import random\n",
    "\n",
    "# for parsing HTML content\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# for progress bars during scraping\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for multithreading (processing URL chunks)\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b38e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    # identify the request as from a real browser\n",
    "    \"User-Agent\": \n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "    \n",
    "    # ask server to respond in english\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08454565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_details(soup):\n",
    "    \"\"\"Extract all key-value product details and build a readable description.\"\"\"\n",
    "    \n",
    "    # dictionary to store extracted product attributes\n",
    "    details = {}\n",
    "    \n",
    "    # find the main container holding product specifications\n",
    "    detail_section = soup.find(\"div\", class_=\"Cnl9Jt\")\n",
    "    \n",
    "    if detail_section:\n",
    "        # each specification row is inside <div class=\"row\">\n",
    "        rows = detail_section.find_all(\"div\", class_=\"row\")\n",
    "        \n",
    "        # iterate through each row to extract keyâ€“value pairs\n",
    "        for row in rows:\n",
    "            \n",
    "            # each row contains exactly two columns:\n",
    "            # - key column: class 'col-3-12'\n",
    "            # - value column: class 'col-9-12'\n",
    "            cols = row.find_all(\"div\", class_=[\"col-3-12\", \"col-9-12\"])\n",
    "            \n",
    "            # only process rows that contain both key and value\n",
    "            if len(cols) == 2:\n",
    "                \n",
    "                # extract text for key and value, removing extra whitespace\n",
    "                key = cols[0].get_text(strip=True)\n",
    "                val = cols[1].get_text(strip=True)\n",
    "                \n",
    "                # store in dictionary\n",
    "                details[key] = val\n",
    "\n",
    "    # convert the extracted details dictionary into a single readable string\n",
    "    if details:\n",
    "        \n",
    "        # join key-value pairs into: \"Key: Value; Key: Value; ...\"\n",
    "        description = \"; \".join([f\"{k}: {v}\" for k, v in details.items()])\n",
    "    else:\n",
    "        # if no details found, return None\n",
    "        description = None\n",
    "\n",
    "    # return the final formatted description text\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_product(url):\n",
    "    \"\"\"Scrape a single Flipkart product page.\"\"\"\n",
    "    try:\n",
    "        # send GET request with headers to avoid bot-blocking\n",
    "        res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "\n",
    "        # if request fails or page not found, return None\n",
    "        if res.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # TITLE\n",
    "        title = soup.find(\"h1\", class_=\"_6EBuvT\")\n",
    "        if title:\n",
    "            title = title.get_text(strip=True)\n",
    "        else:\n",
    "            # same template family -> title can appear here\n",
    "            title = soup.find(\"span\", class_=\"yhB1No\")\n",
    "            title = title.get_text(strip=True) if title else None\n",
    "\n",
    "        # PRICE (MRP)\n",
    "        price = soup.find(\"div\", class_=\"yRaY8j\")\n",
    "        price = price.get_text(strip=True) if price else None\n",
    "\n",
    "        # DISCOUNT %\n",
    "        discount = soup.find(\"div\", class_=\"UkUFwK\")\n",
    "        discount = discount.get_text(strip=True) if discount else None\n",
    "\n",
    "        # DISCOUNTED PRICE (selling price)\n",
    "        discounted_price_el = soup.find(\"div\", class_=\"Nx9bqj\")\n",
    "        discounted_price = discounted_price_el.get_text(strip=True) if discounted_price_el else None\n",
    "\n",
    "        # RATING VALUE\n",
    "        rating_value = soup.find(\"div\", class_=\"XQDdHH\")\n",
    "        rating_value = rating_value.get_text(strip=True) if rating_value else None\n",
    "\n",
    "        # RATING & REVIEW COUNTS\n",
    "        rating_counts, review_counts = None, None\n",
    "        rating_review_text = soup.find(\"span\", class_=\"Wphh3N\")\n",
    "\n",
    "        if rating_review_text:\n",
    "            text = rating_review_text.get_text(strip=True)\n",
    "            parts = text.split(\"and\")\n",
    "            if len(parts) == 2:\n",
    "                rating_counts = parts[0].replace(\"ratings\", \"\").strip()\n",
    "                review_counts = parts[1].replace(\"reviews\", \"\").strip()\n",
    "\n",
    "        # DESCRIPTION\n",
    "        # try custom detail extractor; fallback to a generic text block\n",
    "        description = extract_product_details(soup)\n",
    "        if not description:\n",
    "            desc_block = soup.find(\"div\", class_=\"yN+eNk\")\n",
    "            description = desc_block.get_text(strip=True) if desc_block else None\n",
    "\n",
    "        # HIERARCHY\n",
    "        hierarchy = None\n",
    "        hierarchy_section = soup.find(\"div\", class_=\"DOjaWF\")\n",
    "        if hierarchy_section:\n",
    "            # breadcrumb links\n",
    "            crumbs = hierarchy_section.find_all(\"a\", class_=\"R0cyWM\")\n",
    "            hierarchy_list = [c.get_text(strip=True) for c in crumbs]\n",
    "            hierarchy = \" > \".join(hierarchy_list)\n",
    "\n",
    "            # last element in breadcrumb (non-clickable)\n",
    "            final_text = hierarchy_section.find(\"div\", class_=\"KalC6f\")\n",
    "            if final_text:\n",
    "                hierarchy += \" > \" + final_text.get_text(strip=True)\n",
    "\n",
    "        # RESULT\n",
    "        data = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"price\": price,\n",
    "            \"discount\": discount,\n",
    "            \"discounted_price\": discounted_price,\n",
    "            \"rating_value\": rating_value,\n",
    "            \"rating_counts\": rating_counts,\n",
    "            \"review_counts\": review_counts,\n",
    "            \"hierarchy\": hierarchy,\n",
    "            \"description\": description,\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        # scraping errors or network issues\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d85692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(url):\n",
    "    \"\"\"Scrape one URL, log result, return scraped data.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # attempt to scrape product information from the given URL\n",
    "        data = scrape_product(url)\n",
    "\n",
    "        # random delay to avoid hitting the server too frequently\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        \n",
    "        # if scraping was successful, return the scraped data\n",
    "        if data:\n",
    "            return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        # in case of any scraping or network error, return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694b69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(url_chunk):\n",
    "    \"\"\"Process a chunk of URLs and return list of results.\"\"\"\n",
    "    \n",
    "    chunk_results = []\n",
    "    \n",
    "    # iterate through URLs inside chunk\n",
    "    for url in url_chunk:\n",
    "        try:\n",
    "            # scrape individual URL\n",
    "            data = process_url(url)\n",
    "            \n",
    "            # only append valid scraped results\n",
    "            if data:\n",
    "                chunk_results.append(data)\n",
    "                \n",
    "        # ignore URLs that error out (timeouts, parsing failures)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return chunk_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c86b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 100\n",
    "CHUNK_SIZE = 10\n",
    "OUTPUT_DIR = \"./data/sample-scraped\"\n",
    "file = './data/new-sample/urls_part_2.csv'\n",
    "file_index = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load URLs for this part\n",
    "df = pd.read_csv(file)\n",
    "urls = df['url'].tolist()\n",
    "\n",
    "results = []\n",
    "start = time.time()\n",
    "\n",
    "# split URL list into smaller chunks for parallel processing\n",
    "chunks = [urls[i:i + CHUNK_SIZE] for i in range(0, len(urls), CHUNK_SIZE)]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "\n",
    "    # submit each chunk as a separate task to the thread pool\n",
    "    futures = {executor.submit(process_chunk, chunk): chunk for chunk in chunks}\n",
    "\n",
    "    # iterate over completed tasks\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "\n",
    "        try:\n",
    "            # collect chunk result\n",
    "            data = future.result()\n",
    "            \n",
    "            if data:\n",
    "                # append scraped records\n",
    "                results.extend(data)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # periodic saving\n",
    "        if len(results) > 0 and len(results) % CHUNK_SIZE == 0:\n",
    "            out_file = f\"{OUTPUT_DIR}/flipkart_products_scraped_part_{file_index}.csv\"\n",
    "            pd.DataFrame(results).to_csv(out_file, index=False)\n",
    "            print(f\"Saved {len(results)} records for part {file_index} so far...\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# final save for entire part\n",
    "out_file = f\"{OUTPUT_DIR}/flipkart_products_scraped_part_{file_index}.csv\"\n",
    "pd.DataFrame(results).to_csv(out_file, index=False)\n",
    "\n",
    "print(f\"\\nCompleted Part {file_index}: Scraped {len(results)} products\")\n",
    "print(f\"Time: {round((end - start) / 60, 2)} minutes\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds605-gproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
